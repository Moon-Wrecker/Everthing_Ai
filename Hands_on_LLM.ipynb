{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881af3e5",
   "metadata": {},
   "source": [
    "#  Text Classification\n",
    "\n",
    "* Text Classicfication with Representation Models\n",
    "* Text Classification with Gererative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9ba35",
   "metadata": {},
   "source": [
    "### The Sentiment of Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1353f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "771f64ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0572c4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'things really get weird , though not particularly scary : the movie is all portent and no content .'],\n",
       " 'label': [1, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd90e8",
   "metadata": {},
   "source": [
    "### Text Classification using representation \n",
    " Using a task-specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2162db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc6a862959a42aeb1cda6e0e2780bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98524f7d7645059f3b22450bbe10e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6431db18794476aa79746a41a2ae543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90812e3ae642480cadc0b25c7cb0a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea04d84361f540149090a8c36a78c5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea9eaf44ea7411c93bbd7412a8f6f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    model = model_path,\n",
    "    tokenizer = model_path,\n",
    "    return_all_scores=True,\n",
    "    device = 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ecb5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9e6da8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.pt_utils.KeyDataset at 0x244a0417ce0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KeyDataset(data[\"test\"], \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25aceb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1066/1066 [00:11<00:00, 95.37it/s] \n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total = len(data[\"test\"])):\n",
    "    negative_score = output[0][\"score\"]\n",
    "    positive_score = output[1][\"score\"]\n",
    "    assignment = np.argmax([negative_score, positive_score])\n",
    "    y_pred.append(assignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5091e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.81      0.69      0.75       533\n",
      "Positive Review       0.73      0.84      0.78       533\n",
      "\n",
      "       accuracy                           0.77      1066\n",
      "      macro avg       0.77      0.77      0.77      1066\n",
      "   weighted avg       0.77      0.77      0.77      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"\"Create and print the classification report.\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred, target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "\n",
    "    print(performance)\n",
    "\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e5787",
   "metadata": {},
   "source": [
    "### Classification tasks that leverage embeddings\n",
    "\n",
    " **Supervised learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8748712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7475de69c1dc44ac8983d3349ae06f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8adf7d65c948f2855a787626ea86a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec842057f2384551b5c5d70e61151135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9a695641d84e0785b1d0359f348150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f977b08a37584718893495e074f7b2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2bd2864ef342e895f322d86cfc483c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4acdfd07f054486af7e15909c99d14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f71e5177eb4b149431626302582bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937baf69ef1847f3865d4d4185c15425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c491d5e5809b4f4198eeedb5cf205ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd88d691c9f412495c846a2039094b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e8faea474a4c1f90adb8624fa5f1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c90477036ec4ce89860326a674f870e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True)\n",
    "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6881e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a4acfe",
   "metadata": {},
   "source": [
    "### Model I/O: Loading Quantised models with Langchain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9450fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
      "     ---------------------------------------- 0.0/67.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.8/67.3 MB 12.6 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 4.2/67.3 MB 12.6 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 5.8/67.3 MB 10.4 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 6.6/67.3 MB 8.6 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 7.3/67.3 MB 7.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 8.1/67.3 MB 6.5 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 8.7/67.3 MB 6.1 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 9.2/67.3 MB 5.5 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 9.7/67.3 MB 5.3 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 10.5/67.3 MB 5.0 MB/s eta 0:00:12\n",
      "     ------ --------------------------------- 11.3/67.3 MB 4.9 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 12.1/67.3 MB 4.8 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 13.1/67.3 MB 4.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 14.2/67.3 MB 4.9 MB/s eta 0:00:11\n",
      "     --------- ------------------------------ 15.5/67.3 MB 4.9 MB/s eta 0:00:11\n",
      "     --------- ------------------------------ 16.8/67.3 MB 5.0 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 18.1/67.3 MB 5.1 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 19.7/67.3 MB 5.2 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 21.2/67.3 MB 5.3 MB/s eta 0:00:09\n",
      "     ------------- -------------------------- 22.5/67.3 MB 5.3 MB/s eta 0:00:09\n",
      "     ------------- -------------------------- 23.3/67.3 MB 5.3 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 23.9/67.3 MB 5.2 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 24.4/67.3 MB 5.0 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 24.9/67.3 MB 4.9 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 25.4/67.3 MB 4.8 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 26.2/67.3 MB 4.8 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 27.0/67.3 MB 4.7 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 27.8/67.3 MB 4.7 MB/s eta 0:00:09\n",
      "     ----------------- ---------------------- 28.8/67.3 MB 4.7 MB/s eta 0:00:09\n",
      "     ----------------- ---------------------- 29.9/67.3 MB 4.7 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 30.9/67.3 MB 4.7 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 32.0/67.3 MB 4.8 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 33.6/67.3 MB 4.8 MB/s eta 0:00:08\n",
      "     -------------------- ------------------- 34.9/67.3 MB 4.9 MB/s eta 0:00:07\n",
      "     --------------------- ------------------ 36.4/67.3 MB 4.9 MB/s eta 0:00:07\n",
      "     ---------------------- ----------------- 37.7/67.3 MB 5.0 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 38.5/67.3 MB 5.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 39.6/67.3 MB 4.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 40.6/67.3 MB 4.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 41.7/67.3 MB 4.9 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 43.0/67.3 MB 5.0 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 44.0/67.3 MB 5.0 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 45.1/67.3 MB 5.0 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 46.1/67.3 MB 5.0 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 47.4/67.3 MB 5.0 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 48.2/67.3 MB 5.0 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 49.0/67.3 MB 4.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 49.5/67.3 MB 4.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 50.1/67.3 MB 4.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 50.9/67.3 MB 4.8 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 51.9/67.3 MB 4.8 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 52.7/67.3 MB 4.8 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 54.0/67.3 MB 4.8 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 55.1/67.3 MB 4.8 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 56.4/67.3 MB 4.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 57.7/67.3 MB 4.9 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 59.0/67.3 MB 4.9 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 60.6/67.3 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 61.1/67.3 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 61.9/67.3 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 62.9/67.3 MB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 64.0/67.3 MB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 65.0/67.3 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  66.3/67.3 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 67.3/67.3 MB 4.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp312-cp312-win_amd64.whl size=4864120 sha256=f321c981d3f9648433cd28e389f02013c81b977ea16081bec60d8c2a26bbc574\n",
      "  Stored in directory: C:\\Users\\ASUS\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-v7rtfwhs\\wheels\\e4\\16\\e0\\1a8e6feea862ac9be1cc74654663a567fafa55caa19329785f\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cb3c67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: microsoft/Phi-3-mini-4k-instruct-gguf. Received error Model path does not exist: microsoft/Phi-3-mini-4k-instruct-gguf [type=value_error, input_value={'model_path': 'microsoft...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3-mini-4k-instruct-gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LlamaCpp\n  Value error, Could not load Llama model from path: microsoft/Phi-3-mini-4k-instruct-gguf. Received error Model path does not exist: microsoft/Phi-3-mini-4k-instruct-gguf [type=value_error, input_value={'model_path': 'microsoft...: None, 'grammar': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path = \"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    n_gpu_layers = -1,\n",
    "    max_token = 500,\n",
    "    n_ctx = 2048,\n",
    "    seed = 42,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878faa4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi! My name is  Manas. What is 1 + 1?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is  Manas. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc2eb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import load_dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "#openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",               # e.g. \"gemini-1.5-flash\" or \"gemini-pro\"\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73ace247",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_template = \"\"\"Answer the following questions as best as you can. you have the access tot he following tools:\n",
    "    {tools}\n",
    "\n",
    "    Use the following format:\n",
    "\n",
    "    Question: the input question you must answer\n",
    "    Thoughts: you should always think about what to do\n",
    "    Action: the action to take, should be one of [{tool_names}]\n",
    "    Action Input: the input to the action\n",
    "    Observation: the result of the action\n",
    "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "    Thought : I now know the final answer\n",
    "    Final Answer: the final answer to the original input question\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Question: {input}\n",
    "    Thought: {agent_scratchpad}\n",
    "\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4a76942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89d63e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3483332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, load_tools\n",
    "from langchain.tools import DuckDuckGoSearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6c38ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d45dd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\"], llm=gemini_llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7faaaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "70b17149",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    llm=gemini_llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77144eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the current price of a MacBook Pro in USD? How much would it cost in ERU if the exchange rate is 0.85 ERU for 1 USD.\n",
      "Thoughts: I need to find the current price of a MacBook Pro in USD. Then I can convert that price to ERU using the given exchange rate.  I can use DuckDuckGo to search for the price. Since there are various MacBook Pro models (different screen sizes, processors, storage), I'll try to find a base model price.\n",
      "Action: duckduck\n",
      "Action Input: \"price of 13-inch macbook pro\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: The base 13-inch configuration with a 256GB SSD started at $1,299, matching the retail entry price of the Intel 13-inch MacBook Pro. The 13-inch MacBook Pro is the last one with a Touch Bar., title: Best MacBook Pro Deals for April 2025 | Save up to $1,600 - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: Retail price starts at $1,299; M1 MacBook Pro 13-inch (Late 2020) The M1 model will still be available from select retailers. Apple's late 2020 model with Apple Silicon was a big leap over its ..., title: Best MacBook Pro 13-inch Deals - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-13-inch-deals, snippet: MacBook Pro Price Comparisons - See what a MacBook Pro is worth and sell it for a good price. Values from buyback stores, online markets. ... MacBook Pro (13-inch, 2020) M1: $242.00: $362.00: MacBook Pro (13-inch, 2020) Core i5: $158.00: $273.00: MacBook Pro (16-inch, 2019) Core i7: $229.00:, title: MacBook Pro prices, trade in values & places to sell - Flipsy, link: https://flipsy.com/article/1950/macbook-pro-price, snippet: MacBook Pro deals are readily available on the new 2020 MacBook Pro 13-inch, saving shoppers a substantial amount of money off Apple's MSRP. Select Apple resellers are also discounting AppleCare when purchased with the 13-inch laptops, with exclusive promo code discounts at your fingertips in the AppleInsider 13-inch MacBook Pro Price Guide., title: Best Price on MacBook Pro 13 Inch - Latest Deals, Discounts - AppleInsider, link: https://prices.appleinsider.com/macbook-pro-13-inch-2020\u001b[0m\u001b[32;1m\u001b[1;3mThought: The search results indicate a price of $1299 for a base 13-inch MacBook Pro. Now I need to convert this to ERU using the exchange rate of 0.85.\n",
      "Action: Calculator\n",
      "Action Input: 1299 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1104.1499999999999\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now have the price in USD and the converted price in ERU.\n",
      "Final Answer: A base 13-inch MacBook Pro costs approximately $1299.  This is equal to approximately 1104.15 ERU.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in ERU if the exchange rate is 0.85 ERU for 1 USD.',\n",
       " 'output': 'A base 13-inch MacBook Pro costs approximately $1299.  This is equal to approximately 1104.15 ERU.'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in ERU if the exchange rate is 0.85 ERU for 1 USD.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001: ['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001: ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001: ['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-vision-latest: ['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision: ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest: ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-001: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro-002: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro: ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest: ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-001-tuning: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/gemini-1.5-flash: ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0827: ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0924: ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-exp-03-25: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.5-pro-preview-03-25: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.5-flash-preview-04-17: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-exp: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-001: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-exp-image-generation: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite-preview: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-pro-exp: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-pro-exp-02-05: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-exp-1206: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-thinking-exp: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/learnlm-1.5-pro-experimental: ['generateContent', 'countTokens']\n",
      "models/learnlm-2.0-flash-experimental: ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it: ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it: ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it: ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it: ['generateContent', 'countTokens']\n",
      "models/embedding-001: ['embedContent']\n",
      "models/text-embedding-004: ['embedContent']\n",
      "models/gemini-embedding-exp-03-07: ['embedContent', 'countTextTokens']\n",
      "models/gemini-embedding-exp: ['embedContent', 'countTextTokens']\n",
      "models/aqa: ['generateAnswer']\n",
      "models/imagen-3.0-generate-002: ['predict']\n",
      "models/gemini-2.0-flash-live-001: ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "# import google.generativeai as genai\n",
    "# import os\n",
    "# \n",
    "# # 1) point at your key\n",
    "# genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "# \n",
    "# # 2) fetch & print each model’s name + supported methods\n",
    "# for m in genai.list_models():\n",
    "#     # use the correct snake-case field name\n",
    "#     methods = getattr(m, \"supported_generation_methods\", None)\n",
    "#     print(f\"{m.name}: {methods}\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c59c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d436d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
